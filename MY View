Installed necessary libraries: We installed libraries like langchain_community, langchainhub, chromadb, langchain, langchain-openai, and langchain_google_genai to work with LangChain and Google's Gemini API.
Set up Gemini API key: We added code to fetch your Gemini API key from Colab secrets and set it as an environment variable.
Loaded and split documents: We used WebBaseLoader to load content from a Wikipedia page about World War II and then used RecursiveCharacterTextSplitter to divide the content into smaller, manageable chunks (texts).
Created and stored embeddings: We used GoogleGenerativeAIEmbeddings with the models/embedding-001 model to create vector embeddings of the text chunks and stored them in a Chroma vectorstore.
Set up a retriever: We created a retriever from the Chroma vectorstore to fetch relevant document chunks based on a query.
Pulled a RAG prompt: We pulled a standard RAG prompt from the LangChain hub.
Initialized the Gemini LLM: We initialized a ChatGoogleGenerativeAI model to be used for generating responses. We initially had an issue with the model name but corrected it to use models/gemini-2.5-flash.
Defined a helper function: We created a small function promptJoin to format the retrieved documents for the prompt.
Constructed the RAG chain: We built a RAG chain using LangChain Expression Language (LCEL), combining the retriever, a RunnablePassthrough for the question, the prompt, the LLM, and a string output parser.
Attempted to invoke the RAG chain: We tried to invoke the RAG chain with a question, which led to troubleshooting the model name issue.
Essentially, we've successfully set up the core components for a Retrieval-Augmented Generation system using LangChain, Chroma for vector storage, and the Google Gemini API for embeddings and language model capabilities, although we are still working on resolving the issue with invoking the chain.
